# -*- coding: utf-8 -*-
"""nlp_task3(1,2,2a).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bSfOTeAq83d2Dt02JLrOxdqFhZkM2LMw
"""

import csv
import pandas as pd
import sklearn.model_selection as skm
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score,
                             confusion_matrix,
                             ConfusionMatrixDisplay)
import matplotlib.pyplot as plt

data = pd.read_csv('yelp_labelled.txt', header=None, sep='\t')
data.head(5)

import numpy as np
from sklearn.model_selection import cross_val_score
# Assuming you have a DataFrame data with X_text as the input and y as the labels

# Split the data into training, testing, and validation sets
X_text = data[0]
y = data[1]

X_train, X_test, y_train, y_test = skm.train_test_split(X_text, y, test_size=0.2, stratify=y, random_state=0)

# Define different configurations
vectorizer_configs = [
    {'min_df': 1, 'stop_words': 'english', 'binary': False, 'lowercase': True},
    {'min_df': 2, 'stop_words': None, 'binary': True, 'lowercase': False},
    {'min_df': 3, 'stop_words': None, 'binary': False, 'lowercase': True},
    {'min_df': 1, 'stop_words': None, 'binary': True, 'lowercase': False}
]

alpha_values = [0.1, 1.0, 10.0,100.0]

# # Iterate through different configurations
# for i, config in enumerate(vectorizer_configs):
#     # Create CountVectorizer with current configuration
#     vectorizer = CountVectorizer(min_df=config['min_df'], stop_words=config['stop_words'],
#                                  binary=config['binary'], lowercase=config['lowercase'])

#     # Transform the training and testing data
#     X_train_transformed = vectorizer.fit_transform(X_train)
#     X_test_transformed = vectorizer.transform(X_test)

#     # Iterate through different alpha values for MultinomialNB using cross-validation
#     for alpha in alpha_values:
#         # Create MultinomialNB with current alpha
#         clf = MultinomialNB(alpha=alpha)

#         # Combine training and testing data for cross-validation

#         # Perform cross-validation
#         cv_scores = cross_val_score(clf, X_train_transformed, y_train, cv=6, scoring='accuracy')

#         # Calculate and print mean cross-validation accuracy
#         mean_accuracy = np.mean(cv_scores)
#         print(f'Configuration {i + 1}, Alpha={alpha}: Cross-validation mean accuracy: %.3f' % mean_accuracy)

best_config_index = 1
best_alpha = 1.0

# Use the best configuration to transform both training and testing data
best_config = vectorizer_configs[best_config_index]
best_vectorizer = CountVectorizer(min_df=best_config['min_df'], stop_words=best_config['stop_words'],
                                  binary=best_config['binary'], lowercase=best_config['lowercase'])

X_train_transformed = best_vectorizer.fit_transform(X_train)
X_test_transformed = best_vectorizer.transform(X_test)

# Train the final model with the best alpha on the entire training set
final_model = MultinomialNB(alpha=best_alpha)
final_model.fit(X_train_transformed, y_train)

# Make predictions on the test set
y_pred = final_model.predict(X_test_transformed)

# Evaluate the performance on the test set
test_accuracy = accuracy_score(y_test, y_pred)
print(f'Best Configuration {best_config_index + 1}, Alpha={best_alpha}: Test set accuracy: %.3f' % test_accuracy)

incorrect_indices = []

# Compare predicted labels with true labels
for i, (true_label, predicted_label) in enumerate(zip(y_test, y_pred)):
    if true_label != predicted_label:
        incorrect_indices.append(i)

# Print out misclassified examples
print("Misclassified Examples:")
for index in incorrect_indices:
    print(f"True Label: {y_test.tolist()[index]}, Predicted Label: {y_pred.tolist()[index]}, Text: {X_test.tolist()[index]}")

disp = ConfusionMatrixDisplay.from_estimator(
        final_model,
        X_test_transformed,
        y_test,
        cmap=plt.cm.Blues,
    )

C_values = [0.1, 1.0, 10.0, 100.0]
kernel_types = ['linear', 'rbf', 'poly']

# Iterate through different configurations
# for i, config in enumerate(vectorizer_configs):
#     # Create CountVectorizer with current configuration
#     vectorizer = CountVectorizer(min_df=config['min_df'], stop_words=config['stop_words'],
#                                  binary=config['binary'], lowercase=config['lowercase'])

#     # Transform the training and testing data
#     X_train_transformed = vectorizer.fit_transform(X_train)
#     X_test_transformed = vectorizer.transform(X_test)

#     # Iterate through different kernel types and C values for SVC using cross-validation
#     for kernel_type in kernel_types:
#         for C_value in C_values:
#             # Create SVC with current kernel type and C value
#             clf = SVC(C=C_value, kernel=kernel_type)

#             # Perform cross-validation
#             cv_scores = cross_val_score(clf, X_train_transformed, y_train, cv=6, scoring='accuracy')

#             # Calculate and print mean cross-validation accuracy
#             mean_accuracy = np.mean(cv_scores)
#             print(f'Configuration {i + 1}, Kernel={kernel_type}, C={C_value}: Cross-validation mean accuracy: %.3f' % mean_accuracy)

best_config_index = 2
best_kernel = 'linear'
best_C = 1.0
best_config = vectorizer_configs[best_config_index]
best_vectorizer = CountVectorizer(min_df=best_config['min_df'], stop_words=best_config['stop_words'],
                                  binary=best_config['binary'], lowercase=best_config['lowercase'])

X_train_transformed = best_vectorizer.fit_transform(X_train)
X_test_transformed = best_vectorizer.transform(X_test)

# Train the final model with the best C parameter on the entire training set
final_model = SVC(C=best_C, kernel=best_kernel)  # You can adjust the kernel based on your needs
final_model.fit(X_train_transformed, y_train)

# Make predictions on the test set
y_pred = final_model.predict(X_test_transformed)

# Evaluate the performance on the test set
test_accuracy = accuracy_score(y_test, y_pred)
print(f'Best Configuration {best_config_index + 1}, C={best_C}: Test set accuracy: %.3f' % test_accuracy)

incorrect_indices = []

# Compare predicted labels with true labels
for i, (true_label, predicted_label) in enumerate(zip(y_test, y_pred)):
    if true_label != predicted_label:
        incorrect_indices.append(i)

# Print out misclassified examples
print("Misclassified Examples:")
for index in incorrect_indices:
    print(f"True Label: {y_test.tolist()[index]}, Predicted Label: {y_pred.tolist()[index]}, Text: {X_test.tolist()[index]}")

disp = ConfusionMatrixDisplay.from_estimator(
        final_model,
        X_test_transformed,
        y_test,
        cmap=plt.cm.Blues,
    )