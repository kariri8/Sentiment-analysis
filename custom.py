# -*- coding: utf-8 -*-
"""nlp_task3(3,4,4a).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XtQPUfGPwjRkwCZ-eMd9C_Jv7nN_syY2
"""

import csv
import pandas as pd
import sklearn.model_selection as skm
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import (accuracy_score,
                             confusion_matrix,
                             ConfusionMatrixDisplay)
import matplotlib.pyplot as plt

data = pd.read_csv('yelp_labelled.txt', header=None, sep='\t')
data.head(5)

import numpy as np
from sklearn.model_selection import cross_val_score
# Assuming you have a DataFrame data with X_text as the input and y as the labels

# Split the data into training, testing, and validation sets
X_text = data[0]
y = data[1]

X_train, X_test, y_train, y_test = skm.train_test_split(X_text, y, test_size=0.2, stratify=y, random_state=0)

X_doc = "\n\n".join(X_train)

lemmas = []
pos_tags = []
polarity_scores = []

# Read the SentiWords_1.1 file
with open("SentiWords_1.1.txt", "r") as file:
    for line in file:
        line = line.strip()
        if not line.startswith("#") and line:
            parts = line.split("\t")
            lemma_pos = parts[0].split("#")
            lemma = lemma_pos[0]
            pos_tag = lemma_pos[1]
            polarity_score = float(parts[1])
            lemmas.append(lemma)
            pos_tags.append(pos_tag)
            polarity_scores.append(polarity_score)

# Create a pandas DataFrame
df = pd.DataFrame({
    "Lemma": lemmas,
    "POS": pos_tags,
    "Polarity Score": polarity_scores
})

# Display the DataFrame
df.tail()

!pip install stanza

X_train_list = X_train.tolist()

print(X_train_list)

import stanza
nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma', tokenize_no_ssplit=True)
doc = nlp(X_doc)

pos_mapping = {
    'NOUN': 'n',
    'VERB': 'v',
    'ADJ': 'a',
    'ADV': 'r'
}

nlp2 = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')
doc2 = nlp2('Abandon.')
for sentence in doc2.sentences:
  sum_score = 0
  for word in sentence.words:
    lemma = word.lemma
    pos = pos_mapping.get(word.pos, None)
    if pos:
      word_info = df[(df['Lemma'] == lemma) & (df['POS'] == pos)]

      if not word_info.empty:
                    polarity_score = word_info.iloc[0]['Polarity Score']
                    sum_score += polarity_score
    print(sum_score)

    # word_info = df[df['Lemma'] == lemma]
    # if not word_info.empty:

y_train_list = y_train.tolist()

# Mapping dictionary from Stanza POS tags to SentiWord POS tags

threshold_values = np.linspace(0.0, 1.0, num=20)  # Generate 100 threshold values from 0.0 to 10.0

for threshold in threshold_values:
    correct_predictions = 0

    for sentence in doc.sentences:
        sum_score = 0

        for word in sentence.words:
            lemma = word.lemma
            stanza_pos = word.pos

            # Convert Stanza POS tag to SentiWord POS tag
            sentiword_pos = pos_mapping.get(stanza_pos, None)
            if sentiword_pos:
                # Search for lemma and converted POS in df
                word_info = df[(df['Lemma'] == lemma) & (df['POS'] == sentiword_pos)]

                if not word_info.empty:
                    polarity_score = word_info.iloc[0]['Polarity Score']
                    sum_score += polarity_score
                # If not found, add 0

        # Label the sentence based on sum_score and threshold
        # print("sum_score is ", sum_score)
        predicted_label = 1 if sum_score > threshold else 0


        # Compare to original label
        true_label = y_train_list[doc.sentences.index(sentence)]
        # print(sentence, " predicted:", predicted_label, ", true:", true_label)

        if predicted_label == true_label:
            correct_predictions += 1
    # print("number of correct predictions: ", correct_predictions)
    accuracy = correct_predictions / len(doc.sentences)
    print(f"Threshold: {threshold:.2f}, Accuracy: {accuracy:.4f}")

final_threshold = 0.6
X_test_doc = "\n\n".join(X_test)
y_test_list = y_test.tolist()
nlp_test = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma', tokenize_no_ssplit=True)
doc_test = nlp(X_test_doc)

X_test_list  = X_test.tolist()

final_threshold = 0.63
correct_predictions = 0
for sentence in doc_test.sentences:
        sum_score = 0

        for word in sentence.words:
            lemma = word.lemma
            stanza_pos = word.pos

            # Convert Stanza POS tag to SentiWord POS tag
            sentiword_pos = pos_mapping.get(stanza_pos, None)
            if sentiword_pos:
                # Search for lemma and converted POS in df
                word_info = df[(df['Lemma'] == lemma) & (df['POS'] == sentiword_pos)]

                if not word_info.empty:
                    polarity_score = word_info.iloc[0]['Polarity Score']
                    sum_score += polarity_score
                # If not found, add 0

        # Label the sentence based on sum_score and threshold
        # print("sum_score is ", sum_score)
        predicted_label = 1 if sum_score > final_threshold else 0


        # Compare to original label
        true_label = y_test_list[doc_test.sentences.index(sentence)]
        # print(sentence, " predicted:", predicted_label, ", true:", true_label)

        if predicted_label == true_label:
            correct_predictions += 1
        else:
            print(X_test_list[doc_test.sentences.index(sentence)], " true: ", true_label, " predicted: ", predicted_label)
accuracy = correct_predictions / len(doc_test.sentences)
print(f"Accuracy: {accuracy:.4f}")

